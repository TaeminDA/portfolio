{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rest codes from horses or humans.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yJMp-Y14_IO"
      },
      "source": [
        "# Rest Codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WIYSyn5JZ_"
      },
      "source": [
        "## TPU Stragegy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbTk2p334ozI"
      },
      "source": [
        "# Detect hardware\n",
        "try:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address) # TPU detection\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu) \n",
        "  # Going back and forth between TPU and host is expensive.\n",
        "  # Better to run 128 batches on the TPU before reporting back.\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "  print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "except ValueError:\n",
        "  print('TPU failed to initialize.')\n",
        "\n",
        "\n",
        "with strategy.scope()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRb_DYa5V-v"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X22-AtdE5Xuo"
      },
      "source": [
        "# Download and prepare the horses or humans dataset\n",
        "\n",
        "splits, info = tfds.load('horses_or_humans', as_supervised=True, with_info=True, split=['train[:80%]', 'train[80%:]', 'test'])\n",
        "\n",
        "(train_examples, validation_examples, test_examples) = splits\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes\n",
        "\n",
        "#####\n",
        "\n",
        "def format_image(image, label):\n",
        "  image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
        "  return  image, label\n",
        "\n",
        "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "test_batches = test_examples.map(format_image).batch(1)\n",
        "\n",
        "#####\n",
        "\n",
        "SIZE = 141 #@param {type:\"slider\", min:64, max:300, step:1}\n",
        "BATCH_SIZE = 32 #@param {type:\"integer\"}\n",
        "IMAGE_SIZE = (SIZE, SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob_w_5N267Fa"
      },
      "source": [
        "## Rescale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLn3YrpV65-h"
      },
      "source": [
        "# Another way to rescale img\n",
        "\n",
        "def format_image(data):        \n",
        "   image = data[\"image\"]\n",
        "   image = tf.reshape(image, [-1])\n",
        "   image = tf.cast(image, 'float32')\n",
        "   image = image / 255.0\n",
        "   return image, data[\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3GhG80H5eyz"
      },
      "source": [
        "## Custom Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yS6TBPu5lsO"
      },
      "source": [
        "### Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wIIAH5R5gFv"
      },
      "source": [
        "optimizer = optimizer = tf.keras.optimizers.RMSprop()\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kxnTA7S5m7J"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX_7Xj_c5jtU"
      },
      "source": [
        "def train_data_for_one_epoch():\n",
        "\n",
        "  losses = []\n",
        "  pbar = tqdm(total=len(list(enumerate(train_generator))), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ')\n",
        "\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_generator):\n",
        "\n",
        "    logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)\n",
        "    losses.append(loss_value)\n",
        "\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "    pbar.set_description(\"Training loss for step %s: %.4f\" % (int(step), float(loss_value)))\n",
        "    pbar.update()\n",
        "\n",
        "  return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_yxPFnP5o-B"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfKTNesr5pis"
      },
      "source": [
        "def perform_validation():\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  for x_val, y_val in validation_generator:\n",
        "\n",
        "    val_logits = model(x_val)\n",
        "    val_loss = loss_object(y_true=y_val, y_pred=val_logits)\n",
        "    losses.append(val_loss)\n",
        "\n",
        "    val_acc_metric(y_val, val_logits)\n",
        "\n",
        "  return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf8ox_Yc5o3o"
      },
      "source": [
        "### Running the Final Model with Customized Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2dQEUnd5wgN"
      },
      "source": [
        "model = My_Custom_Model()\n",
        "\n",
        "epochs = 3\n",
        "epochs_val_losses, epochs_train_losses = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "  \n",
        "  losses_train = train_data_for_one_epoch()\n",
        "  train_acc = train_acc_metric.result()\n",
        "\n",
        "  losses_val = perform_validation()\n",
        "  val_acc = val_acc_metric.result()\n",
        "\n",
        "  losses_train_mean = np.mean(losses_train)\n",
        "  losses_val_mean = np.mean(losses_val)\n",
        "  epochs_val_losses.append(losses_val_mean)\n",
        "  epochs_train_losses.append(losses_train_mean)\n",
        "\n",
        "  print('\\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch, float(losses_train_mean), float(losses_val_mean), float(train_acc), float(val_acc)))\n",
        "  \n",
        "  train_acc_metric.reset_states()\n",
        "  val_acc_metric.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}